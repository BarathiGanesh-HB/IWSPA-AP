{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths of the input files\n",
    "no_head_train_path_0 = '../data/IWSPA-AP-traindata/phish/'\n",
    "no_head_train_path_1 = '../data/IWSPA-AP-traindata/legit/'\n",
    "head_train_path_0 = '../data/Dataset_Full_Header_Training/Dataset_Submit_Phish/'\n",
    "head_train_path_1 = '../data/Dataset_Full_Header_Training/Dataset_Submit_Legit/'\n",
    "no_head_test_path = '../data/IWSPA-APTestData/testdata_noheaders/'\n",
    "head_test_path = '../data/IWSPA-APTestData/testdata_fullheaders/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string\n",
    "import numpy as np\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.decode('utf-8')\n",
    "    while '\\n' in text:\n",
    "        text = text.replace('\\n', ' ')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    words = text.split()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    stripped = []\n",
    "    for token in words: \n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            stripped.append(new_token.lower())\n",
    "    text = ' '.join(stripped)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    text_list = list()\n",
    "    files = os.listdir(path)\n",
    "    for text_file in files:\n",
    "        file_path = os.path.join(path, text_file)\n",
    "        read_file = open(file_path,'r+')\n",
    "        read_text = read_file.read()\n",
    "        read_file.close()\n",
    "        cleaned_text = clean_text(read_text)\n",
    "        text_list.append(cleaned_text)\n",
    "    return text_list, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train_0, temp = get_data(no_head_train_path_0)\n",
    "no_head_train_1, temp = get_data(no_head_train_path_1)\n",
    "head_train_0, temp = get_data(head_train_path_0)\n",
    "head_train_1, temp = get_data(head_train_path_1)\n",
    "no_head_test, no_head_files = get_data(no_head_test_path)\n",
    "head_test, head_files = get_data(head_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train = no_head_train_0 + no_head_train_1\n",
    "no_head_labels_train = ([0] * len(no_head_train_0)) + ([1] * len(no_head_train_1))\n",
    "\n",
    "head_train = head_train_0 + head_train_1\n",
    "head_labels_train = ([0] * len(head_train_0)) + ([1] * len(head_train_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer()\n",
    "X = tf_vectorizer.fit_transform(no_head_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('#total words', np.matrix.sum(X.todense()))\n",
    "print ('#unique words',len(set(tf_vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(head_train)\n",
    "X = tf_vectorizer.fit_transform(head_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('#total words', np.matrix.sum(X.todense()))\n",
    "print ('#unique words',len(set(tf_vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(len(no_head_labels_train))\n",
    "train_data = np.array(no_head_train)[shuffled_indices]\n",
    "train_data = train_data.tolist()\n",
    "train_label = np.array(no_head_labels_train)[shuffled_indices]\n",
    "train_label = train_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_data = train_data[0:int(0.8*len(train_data))]\n",
    "temp_train_label = train_label[0:int(0.8*len(train_label))]\n",
    "temp_test_data = train_data[int(0.8*len(train_data)):]\n",
    "temp_test_labels = train_label[int(0.8*len(train_label)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_train_file = '../data/fast_train.txt'\n",
    "fast_test_file = '../data/fast_test.txt'\n",
    "writeFile = open(fast_train_file, 'w')\n",
    "for text, label in zip(temp_train_data, temp_train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()\n",
    "\n",
    "writeFile = open(fast_test_file, 'w')\n",
    "for text, label in zip(temp_test_data, temp_test_labels):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(fast_train_file, '../models/Amrita-NLP_TOP_fastText_noheaders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.test(fast_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.precision, result.recall, result.nexamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classifier.min_count, classifier.dim, classifier.epoch, classifier.word_ngrams, classifier.encoding, classifier.loss_name, classifier.maxn, classifier.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fast_train_file = '../data/final_fast_train.txt'\n",
    "writeFile = open(final_fast_train_file, 'w')\n",
    "for text, label in zip(train_data, train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(final_fast_train_file, '../models/Amrita-NLP_TOP_fastText_noheaders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(no_head_test)):\n",
    "    if len(no_head_test[i]) == 0:\n",
    "        no_head_test[i] = '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = classifier.predict(no_head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open('../submission/Amrita-NLP_submission_TOP_noheaders_1.txt', 'w')\n",
    "for value, test_file in zip(final_labels,no_head_files):\n",
    "    writeFile.write(test_file + ' ' + value[0])\n",
    "    writeFile.write('\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(len(head_labels_train))\n",
    "train_data = np.array(head_train)[shuffled_indices]\n",
    "train_data = train_data.tolist()\n",
    "train_label = np.array(head_labels_train)[shuffled_indices]\n",
    "train_label = train_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_data = train_data[0:int(0.8*len(train_data))]\n",
    "temp_train_label = train_label[0:int(0.8*len(train_label))]\n",
    "temp_test_data = train_data[int(0.8*len(train_data)):]\n",
    "temp_test_labels = train_label[int(0.8*len(train_label)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_train_file = '../data/fast_train.txt'\n",
    "fast_test_file = '../data/fast_test.txt'\n",
    "writeFile = open(fast_train_file, 'w')\n",
    "for text, label in zip(temp_train_data, temp_train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()\n",
    "\n",
    "writeFile = open(fast_test_file, 'w')\n",
    "for text, label in zip(temp_test_data, temp_test_labels):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(fast_train_file, '../models/Amrita-NLP_TOP_fastText_headers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.test(fast_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.precision, result.recall, result.nexamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fast_train_file = '../data/final_fast_train.txt'\n",
    "writeFile = open(final_fast_train_file, 'w')\n",
    "for text, label in zip(train_data, train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(final_fast_train_file, '../models/Amrita-NLP_TOP_fastText_headers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = classifier.predict(head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open('../submission/Amrita-NLP_submission_TOP_headers_1.txt', 'w')\n",
    "for value, test_file in zip(final_labels, head_files):\n",
    "    writeFile.write(test_file + ' ' + value[0])\n",
    "    writeFile.write('\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head_train = no_head_train_0 + no_head_train_1\n",
    "no_head_labels_train = ([0] * len(no_head_train_0)) + ([1] * len(no_head_train_1))\n",
    "\n",
    "head_train = head_train_0 + head_train_1\n",
    "head_labels_train = ([0] * len(head_train_0)) + ([1] * len(head_train_1))\n",
    "\n",
    "temp_train = no_head_train + head_train\n",
    "temp_labels = no_head_labels_train + head_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(len(temp_labels))\n",
    "train_data = np.array(temp_train)[shuffled_indices]\n",
    "train_data = train_data.tolist()\n",
    "train_label = np.array(temp_labels)[shuffled_indices]\n",
    "train_label = train_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_data = train_data[0:int(0.8*len(train_data))]\n",
    "temp_train_label = train_label[0:int(0.8*len(train_label))]\n",
    "temp_test_data = train_data[int(0.8*len(train_data)):]\n",
    "temp_test_labels = train_label[int(0.8*len(train_label)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_train_file = '../data/fast_train.txt'\n",
    "fast_test_file = '../data/fast_test.txt'\n",
    "writeFile = open(fast_train_file, 'w')\n",
    "for text, label in zip(temp_train_data, temp_train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()\n",
    "\n",
    "writeFile = open(fast_test_file, 'w')\n",
    "for text, label in zip(temp_test_data, temp_test_labels):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(fast_train_file, '../models/model_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.test(fast_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.precision, result.recall, result.nexamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fast_train_file = '../data/final_fast_train.txt'\n",
    "writeFile = open(final_fast_train_file, 'w')\n",
    "for text, label in zip(train_data, train_label):\n",
    "    writeFile.write('__label__'+str(label)+' '+str(text.encode('utf-8'))+'\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised(final_fast_train_file, '../models/model_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(no_head_test)):\n",
    "    if len(no_head_test[i]) == 0:\n",
    "        no_head_test[i] = '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_head_labels = classifier.predict(head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_no_head_labels = classifier.predict(no_head_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open('../submission/Amrita-NLP_submission_headers_2.txt', 'w')\n",
    "for value, test_file in zip(final_head_labels , head_files):\n",
    "    writeFile.write(test_file + ' ' + value[0])\n",
    "    writeFile.write('\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open('../submission/Amrita-NLP_submission_noheaders_2.txt', 'w')\n",
    "for value, test_file in zip(final_no_head_labels,no_head_files):\n",
    "    writeFile.write(test_file + ' ' + value[0])\n",
    "    writeFile.write('\\n')\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IWSPA-AP",
   "language": "python",
   "name": "iwspa-ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
